#!/usr/bin/env python3
"""
Zotero Linked-File Importer (Best-Practices Edition)

Creates properly structured Zotero items with linked-file attachments using the
canonical literature manifest for the KANNA project.

Best-practice improvements over the initial version:
  - Creates parent bibliographic items (journalArticle) instead of orphaned attachments
  - Uses Linked Attachment Base Directory friendly paths (attachments:) when requested
  - Optional DOI extraction from filenames; stores DOI in parent item if detected
  - Idempotent deduplication by file path, DOI, or normalized title+year
  - Batch creation with size guards; dry-run mode for safe previews

Requirements:
  - Environment variables:
      * ZOTERO_API_KEY       (write-enabled key for the target library)
      * ZOTERO_LIBRARY_ID    (numeric Zotero library ID)
      * ZOTERO_LIBRARY_TYPE  (default: "user")
      * ZOTERO_BASE_DIR      (optional; absolute path matching Zotero Linked Attachment Base Directory)

  - `manifest.csv` generated by `generate_literature_manifest.py`
    located in `literature/pdfs/BIBLIOGRAPHIE/`.

The script is idempotent:
  * Reuses existing collections
  * Updates existing items/attachments (collections/tags/parent linkage)
  * Skips duplicates cleanly; reports missing files without aborting
"""

from __future__ import annotations

import argparse
import csv
import os
import re
import sys
from collections import defaultdict
import time
import random
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from pyzotero import zotero
try:
    import requests  # for HTTPError status codes
except Exception:  # pragma: no cover
    requests = None  # type: ignore

PROJECT_ROOT = Path(__file__).resolve().parents[2]
BIBLIO_DIR = PROJECT_ROOT / "literature" / "pdfs" / "BIBLIOGRAPHIE"
DEFAULT_MINERU_DIR = PROJECT_ROOT / "literature" / "pdfs" / "extractions-mineru"
MANIFEST_PATH = BIBLIO_DIR / "manifest.csv"

# Collection display names keyed by manifest chapter label
COLLECTION_NAME_MAP: Dict[str, str] = {
    # Only used if chapter-based subcollections are requested.
    "general": "General",
    "introduction": "Chapter 1: Introduction",
    "botany": "Chapter 2: Botany",
    "ethnobotany": "Chapter 3: Ethnobotany",
    "pharmacology": "Chapter 4: Pharmacology",
    "clinical": "Chapter 5: Clinical",
    "addiction": "Chapter 6: Addiction",
    "legal-ethics": "Chapter 7: Legal/Ethics",
    "synthesis": "Chapter 8: Synthesis",
}


def require_env(name: str) -> str:
    value = os.getenv(name)
    if not value:
        raise SystemExit(f"Environment variable {name} is required.")
    return value


def get_zotero_client() -> zotero.Zotero:
    api_key = require_env("ZOTERO_API_KEY")
    library_id = require_env("ZOTERO_LIBRARY_ID")
    library_type = os.getenv("ZOTERO_LIBRARY_TYPE", "user")
    return zotero.Zotero(library_id, library_type, api_key)


def ensure_collection(
    zot: zotero.Zotero, name: str, parent_key: Optional[str]
) -> str:
    """Return the key for the named collection, creating it if needed."""
    # Use everything() to avoid pagination pitfalls
    for coll in zot.everything(zot.collections()):
        data = coll.get("data", {})
        if data.get("name") == name and (data.get("parentCollection") or "") == (
            parent_key or ""
        ):
            return coll["key"]

    payload = [{"name": name, "parentCollection": parent_key or ""}]
    response = safe_create_collections(zot, payload)
    success_map = response.get("success") or response.get("successful") or {}
    if not success_map:
        raise RuntimeError(f"Failed to create collection '{name}': {response}")

    return next(iter(success_map.values()))


def build_collection_map(
    zot: zotero.Zotero,
    root_name: Optional[str],
    with_chapter_subcollections: bool,
) -> Tuple[Optional[str], Dict[str, str]]:
    """Create collection root and optional chapter subcollections.

    Returns (root_key, mapping_by_chapter_label). If no root_name is provided,
    returns (None, {}).
    """
    if not root_name:
        return None, {}

    parent_key = ensure_collection(zot, root_name, parent_key=None)
    if not with_chapter_subcollections:
        return parent_key, {}

    key_map: Dict[str, str] = {}
    for label, display_name in COLLECTION_NAME_MAP.items():
        key = ensure_collection(zot, display_name, parent_key=parent_key)
        key_map[label] = key

    return parent_key, key_map


def fetch_existing_attachments(
    zot: zotero.Zotero,
) -> Dict[str, Dict[str, object]]:
    """Return existing linked-file attachments keyed by path (absolute or attachments:)."""
    attachments: Dict[str, Dict[str, object]] = {}
    for item in zot.everything(zot.items(itemType="attachment")):
        data = item.get("data", {})
        if data.get("linkMode") == "linked_file" and data.get("path"):
            attachments[str(data["path"])]= item
    return attachments


def fetch_existing_parents(zot: zotero.Zotero) -> Tuple[Dict[str, Dict], Dict[Tuple[str, str], Dict]]:
    """
    Build indices of existing bibliographic items for deduplication:
      - by DOI (uppercased)
      - by (normalized_title, year)
    Returns (by_doi, by_title_year)
    """
    by_doi: Dict[str, Dict] = {}
    by_title_year: Dict[Tuple[str, str], Dict] = {}

    for item in zot.everything(zot.items()):
        data = item.get("data", {})
        if data.get("itemType") in {"attachment", "note"}:
            continue
        doi = (data.get("DOI") or "").strip().upper()
        title = (data.get("title") or "").strip().lower()
        year = ""
        date = (data.get("date") or "").strip()
        if date:
            # Use first 4 consecutive digits as year if present
            m = re.search(r"(\d{4})", date)
            if m:
                year = m.group(1)
        if doi:
            by_doi[doi] = item
        if title and year:
            norm_title = re.sub(r"\s+", " ", re.sub(r"[^a-z0-9]+", " ", title)).strip()
            by_title_year[(norm_title, year)] = item
    return by_doi, by_title_year


def read_manifest() -> List[Dict[str, str]]:
    if not MANIFEST_PATH.exists():
        raise SystemExit(
            f"Manifest not found at {MANIFEST_PATH}. "
            "Run generate_literature_manifest.py first."
        )

    with MANIFEST_PATH.open(newline="", encoding="utf-8") as handle:
        return list(csv.DictReader(handle))


def chapter_tag(chapter_raw: str) -> str:
    try:
        chapter_num = int(chapter_raw)
    except (TypeError, ValueError):
        return "General"
    if chapter_num <= 0:
        return "General"
    return f"Chapter {chapter_num}"


def split_authors_to_creators(authors_field: str) -> List[Dict[str, str]]:
    """Convert a semicolon-separated authors string to Zotero creators list."""
    creators: List[Dict[str, str]] = []
    if not authors_field:
        return creators
    for author in [a.strip() for a in authors_field.split(";") if a.strip()]:
        # Heuristics: try "Last, First" or split on last space
        if "," in author:
            last, first = [p.strip() for p in author.split(",", 1)]
            creators.append({"creatorType": "author", "firstName": first, "lastName": last})
        else:
            parts = author.split()
            if len(parts) >= 2:
                first = " ".join(parts[:-1])
                last = parts[-1]
                creators.append({"creatorType": "author", "firstName": first, "lastName": last})
            else:
                creators.append({"creatorType": "author", "name": author})
    return creators


def normalize_doi(raw: str) -> str:
    doi = raw.strip()
    doi = re.sub(r"^doi:\s*", "", doi, flags=re.I)
    doi = doi.strip().strip("[]{}()")
    # collapse spaces and trailing punctuation
    doi = re.sub(r"\s+", "", doi)
    doi = doi.rstrip(".")
    return doi


def extract_doi_from_filename(filename: str) -> Optional[str]:
    """Extract DOI-like token from filename if present."""
    s = filename.replace(" ", "")
    m = re.search(r"(10\.\d{4,9}/[^\s]+)", s, flags=re.I)
    if m:
        return normalize_doi(m.group(1).rstrip(".pdf"))
    return None


def compute_linked_path(pdf_path: Path, base_dir: Optional[Path]) -> str:
    """Return Zotero attachment path, preferring 'attachments:' form if base_dir is given.
    If base_dir is set and pdf is inside it, path will be 'attachments:relative/path'.
    Otherwise, returns absolute path.
    """
    if base_dir:
        try:
            rel = pdf_path.resolve().relative_to(base_dir.resolve())
            return f"attachments:{rel.as_posix()}"
        except Exception:
            pass
    return str(pdf_path)


def normalize_key(s: str) -> str:
    return re.sub(r"[^a-z0-9]+", "", s.lower())


def build_mineru_index(mineru_dir: Path) -> Dict[str, Path]:
    """Map normalized extraction folder names to their auto/*.md path."""
    index: Dict[str, Path] = {}
    if not mineru_dir.exists():
        return index
    for entry in mineru_dir.iterdir():
        if not entry.is_dir():
            continue
        auto_dir = entry / "auto"
        if not auto_dir.exists():
            continue
        md_files = sorted(auto_dir.glob("*.md"))
        if not md_files:
            continue
        key = normalize_key(entry.name)
        # prefer first md file
        index[key] = md_files[0]
    return index


def try_find_extraction_md_for(pdf_filename: str, mineru_dir: Path, mineru_idx: Optional[Dict[str, Path]] = None) -> Optional[Path]:
    """Locate the MinerU markdown corresponding to a PDF filename using fuzzy matching."""
    stem = Path(pdf_filename).stem
    # Direct prefix match first
    for candidate in sorted((mineru_dir.glob(f"{stem}*/auto/*.md"))):
        return candidate
    relaxed = stem.rstrip(". ")
    for candidate in sorted((mineru_dir.glob(f"{relaxed}*/auto/*.md"))):
        return candidate
    # Fuzzy: normalized key lookup
    if mineru_idx is None:
        mineru_idx = build_mineru_index(mineru_dir)
    key = normalize_key(stem)
    if key in mineru_idx:
        return mineru_idx[key]
    return None


def parse_mineru_metadata(md_path: Path) -> Dict[str, object]:
    """Parse title, authors, abstract, keywords, DOI from a MinerU markdown file."""
    text = md_path.read_text(encoding="utf-8", errors="ignore")

    # Title: first H1
    m_title = re.search(r"^#\s+(.+)$", text, re.MULTILINE)
    title = m_title.group(1).strip() if m_title else md_path.stem

    # Authors: next non-empty line after title
    authors = ""
    if m_title:
        start = m_title.end()
        rest = text[start:]
        m_auth = re.search(r"^(?:\s*)([^\n\r].+)$", rest, re.MULTILINE)
        if m_auth:
            authors = m_auth.group(1).strip()

    # Keywords block
    keywords: List[str] = []
    m_kw = re.search(r"Keywords?:\s*(.+?)(?:\n\n|#)", text, re.DOTALL | re.IGNORECASE)
    if m_kw:
        kw_text = m_kw.group(1)
        keywords = [k.strip() for k in re.split(r"[,;\n]", kw_text) if k.strip()]

    # Abstract heading sometimes spaced: a b s t r a c t
    abstract = ""
    m_abs = re.search(r"#\s*a\s*b\s*s\s*t\s*r\s*a\s*c\s*t\s*\n\n(.+?)(?:\n\n#|\$)", text, re.DOTALL | re.IGNORECASE)
    if m_abs:
        abstract = re.sub(r"\s+", " ", m_abs.group(1).strip())

    # DOI in content
    doi = None
    m_doi = re.search(r"(10\.\d{4,9}/[^\s\)\]\}]+)", text)
    if m_doi:
        doi = normalize_doi(m_doi.group(1))

    return {
        "title": title,
        "authors_text": authors,
        "keywords": keywords,
        "abstract": abstract,
        "doi": doi,
    }


ALKALOIDS = [
    "mesembrine",
    "mesembrenone",
    "mesembrenol",
    "tortuosamine",
    "mesembranol",
    "joubertiamine",
    "sceletine",
]

CONCEPTS = [
    # Pharmacology
    "SERT",
    "PDE4",
    "serotonin",
    "phosphodiesterase",
    "anxiolytic",
    "antidepressant",
    "dopamine",
    "norepinephrine",
    # Methods
    "QSAR",
    "molecular docking",
    "LC-MS",
    "HPLC",
    "NMR",
    "GC-MS",
    "meta-analysis",
    "RCT",
    # Biology / Ethnobotany
    "Sceletium tortuosum",
    "Kanna",
    "Aizoaceae",
    "Mesembryanthemaceae",
    "Khoisan",
    "traditional use",
    "fermentation",
    # Legal/Ethics
    "biopiracy",
    "benefit-sharing",
    "FPIC",
]


def detect_concepts(text: str) -> Tuple[List[str], List[str]]:
    tl = text.lower()
    alk = sorted({a for a in ALKALOIDS if a.lower() in tl})
    con = sorted({c for c in CONCEPTS if c.lower() in tl})
    return alk, con


# -----------------------------
# API hardening helpers
# -----------------------------

def is_http_error_throttle(exc: Exception) -> bool:
    code = getattr(getattr(exc, 'response', None), 'status_code', None)
    if code in (429, 503):
        return True
    # Some libraries use .code
    code = getattr(exc, 'code', None)
    return code in (429, 503)


def safe_sleep_backoff(attempt: int, base_seconds: float = 2.0, cap: float = 60.0) -> None:
    delay = min(cap, base_seconds * (2 ** (attempt - 1)))
    # Add jitter
    delay = delay * (0.75 + random.random() * 0.5)
    time.sleep(delay)


def safe_create_items(zot: zotero.Zotero, items: List[Dict[str, object]], max_retries: int = 5) -> Dict[str, object]:
    attempt = 0
    while True:
        attempt += 1
        try:
            return zot.create_items(items)
        except Exception as e:  # HTTP errors, connection errors
            if attempt >= max_retries or not is_http_error_throttle(e):
                raise
            safe_sleep_backoff(attempt)


def safe_update_item(zot: zotero.Zotero, item: Dict[str, object], max_retries: int = 5) -> Dict[str, object]:
    attempt = 0
    while True:
        attempt += 1
        try:
            return zot.update_item(item)
        except Exception as e:
            if attempt >= max_retries or not is_http_error_throttle(e):
                raise
            safe_sleep_backoff(attempt)


def safe_create_collections(zot: zotero.Zotero, cols: List[Dict[str, object]], max_retries: int = 5) -> Dict[str, object]:
    attempt = 0
    while True:
        attempt += 1
        try:
            return zot.create_collections(cols)
        except Exception as e:
            if attempt >= max_retries or not is_http_error_throttle(e):
                raise
            safe_sleep_backoff(attempt)


def main() -> int:
    parser = argparse.ArgumentParser(description="Import linked PDFs into Zotero with parent items and flexible organization")
    parser.add_argument("--dry-run", action="store_true", help="Show actions without modifying Zotero")
    parser.add_argument("--batch-size", type=int, default=20, help="Items to create per batch (default 20)")
    parser.add_argument("--base-dir", type=str, default=os.getenv("ZOTERO_BASE_DIR", ""), help="Linked Attachment Base Directory for relative paths")
    parser.add_argument("--no-collections", action="store_true", help="Do not create/use any collections")
    parser.add_argument("--collection-root", type=str, default="KANNA Library", help="Root collection name (ignored if --no-collections)")
    parser.add_argument(
        "--collections-mode",
        type=str,
        choices=["root", "chapters"],
        default="root",
        help="Create only a root collection or root+chapter subcollections",
    )
    parser.add_argument(
        "--tagging-mode",
        type=str,
        choices=["none", "basic", "chapter"],
        default="basic",
        help="Tagging strategy: none, basic(project/status/year), or chapter (adds Chapter X)",
    )
    parser.add_argument(
        "--mineru-dir",
        type=str,
        default=str(DEFAULT_MINERU_DIR),
        help="MinerU extractions root for metadata enrichment",
    )
    args = parser.parse_args()

    zot = get_zotero_client()
    root_key = None
    collection_keys: Dict[str, str] = {}
    if not args.no_collections:
        root_key, collection_keys = build_collection_map(
            zot,
            root_name=args.collection_root,
            with_chapter_subcollections=(args.collections_mode == "chapters"),
        )
    existing_by_path = fetch_existing_attachments(zot)
    existing_by_doi, existing_by_title_year = fetch_existing_parents(zot)
    manifest_rows = read_manifest()

    base_dir: Optional[Path] = Path(args.base_dir).resolve() if args.base_dir else None
    mineru_dir = Path(args.mineru_dir)
    mineru_idx = build_mineru_index(mineru_dir) if mineru_dir.exists() else {}

    created_parents = 0
    created_attachments = 0
    updated = 0
    skipped = 0
    missing_files: List[str] = []
    created_by_collection: Dict[str, int] = defaultdict(int)

    # First pass: determine which parents need to be created
    parent_payloads: List[Dict[str, object]] = []
    parent_indices: List[int] = []  # index into manifest_rows
    parent_key_for_row: Dict[int, str] = {}

    # Track attachments to create after parent creation
    pending_attachments: List[Tuple[int, Dict[str, object], str]] = []  # (row_idx, payload, chapter_label)

    for idx, row in enumerate(manifest_rows):
        filename = (row.get("filename") or "").strip()
        if not filename:
            continue

        pdf_path = (BIBLIO_DIR / filename).resolve()
        if not pdf_path.exists():
            missing_files.append(filename)
            continue

        chapter_label = (row.get("chapter_label") or "general").strip() or "general"
        # Collections: either root only, or per chapter if created; or none
        collection_key = None
        if not args.no_collections:
            if args.collections_mode == "chapters" and chapter_label in collection_keys:
                collection_key = collection_keys[chapter_label]
            else:
                collection_key = root_key

        # Tagging baseline
        tags: List[Dict[str, str]] = []
        if args.tagging_mode in {"basic", "chapter"}:
            year_tag = (row.get("year") or "").strip()
            if year_tag:
                tags.append({"tag": f"year:{year_tag}"})
            tags.append({"tag": "project:KANNA"})
            tags.append({"tag": "status:untriaged"})
        if args.tagging_mode == "chapter":
            tags.append({"tag": chapter_tag(row.get("chapter", "0"))})

        # MinerU enrichment (optional)
        mineru_md: Optional[Path] = None
        mineru_meta: Dict[str, object] = {}
        if mineru_dir.exists():
            mineru_md = try_find_extraction_md_for(filename, mineru_dir, mineru_idx)
            if mineru_md and mineru_md.exists():
                mineru_meta = parse_mineru_metadata(mineru_md)

        # Title preference: MinerU > manifest > filename stem
        title = (str(mineru_meta.get("title")) if mineru_meta.get("title") else (row.get("title") or pdf_path.stem)).strip() or pdf_path.stem
        year = (row.get("year") or "").strip()
        authors_field = (row.get("authors") or "").strip()
        if not authors_field and mineru_meta.get("authors_text"):
            authors_field = str(mineru_meta.get("authors_text") or "").strip()
        doi = extract_doi_from_filename(filename) or (str(mineru_meta.get("doi")) if mineru_meta.get("doi") else None)
        if doi:
            doi = normalize_doi(doi)

        # Concept tags from MinerU metadata (title+abstract+keywords)
        if mineru_meta:
            search_text = " ".join([
                str(mineru_meta.get("title") or ""),
                str(mineru_meta.get("abstract") or ""),
                " ".join(mineru_meta.get("keywords") or []),
            ])
            alk, cons = detect_concepts(search_text)
            for a in alk:
                tags.append({"tag": f"alkaloid:{a}"})
            for c in cons:
                tags.append({"tag": f"concept:{c}"})

        # Dedup: if attachment exists by absolute or attachments: path
        abs_path = str(pdf_path)
        rel_or_abs_path = compute_linked_path(pdf_path, base_dir)
        existing_item = existing_by_path.get(abs_path) or existing_by_path.get(rel_or_abs_path)
        if existing_item:
            data = existing_item["data"]
            needs_update = False

            # Ensure title on attachment
            if data.get("title") != title:
                data["title"] = title
                needs_update = True

            # Link to parent if we can find/create one later; we'll update after parents creation
            # For now, just ensure tags/collections
            if collection_key and collection_key not in data.get("collections", []):
                data.setdefault("collections", []).append(collection_key)
                needs_update = True
            if tags:
                existing_tags = {t.get("tag") for t in data.get("tags", [])}
                for t in tags:
                    if t["tag"] not in existing_tags:
                        data.setdefault("tags", []).append(t)
                        needs_update = True

            if needs_update and not args.dry_run:
                safe_update_item(zot, existing_item)
                updated += 1
            else:
                skipped += 1

            # Try to find a parent item; if found and attachment lacks parent, set it
            parent_item = None
            if doi and doi.upper() in existing_by_doi:
                parent_item = existing_by_doi[doi.upper()]
            else:
                if year:
                    norm_title = re.sub(r"\s+", " ", re.sub(r"[^a-z0-9]+", " ", title.lower())).strip()
                    parent_item = existing_by_title_year.get((norm_title, year))
            if parent_item and not data.get("parentItem") and not args.dry_run:
                data["parentItem"] = parent_item["key"]
                safe_update_item(zot, existing_item)
                updated += 1

            continue

        # Find existing parent bibliographic item
        parent_item = None
        if doi and doi.upper() in existing_by_doi:
            parent_item = existing_by_doi[doi.upper()]
        else:
            if year:
                norm_title = re.sub(r"\s+", " ", re.sub(r"[^a-z0-9]+", " ", title.lower())).strip()
                parent_item = existing_by_title_year.get((norm_title, year))

        if parent_item:
            parent_key_for_row[idx] = parent_item["key"]
        else:
            # Queue parent creation
            creators = split_authors_to_creators(authors_field)
            parent = {
                "itemType": "journalArticle",
                "title": title,
                "creators": creators,
                "date": year if year else "",
                "DOI": doi or "",
                "collections": [collection_key] if collection_key else [],
                "tags": tags or [],
            }
            if doi:
                parent["url"] = f"https://doi.org/{doi}"
            # Optional: enrich parent with abstract and MinerU extraction path
            if mineru_meta.get("abstract"):
                parent["abstractNote"] = str(mineru_meta.get("abstract"))
            if mineru_md:
                try:
                    parent["extra"] = f"source: MinerU; extraction: {mineru_md.parent.parent.relative_to(PROJECT_ROOT)}"
                except Exception:
                    parent["extra"] = "source: MinerU"
            parent_payloads.append(parent)
            parent_indices.append(idx)

        # Prepare attachment payload; parentItem will be filled after parent creation if needed
        path_str = rel_or_abs_path
        att = {
            "itemType": "attachment",
            "linkMode": "linked_file",
            "title": title,
            "path": path_str,
            "contentType": "application/pdf",
            "tags": tags or [],
            "collections": [collection_key] if collection_key else [],
        }
        pending_attachments.append((idx, att, chapter_label))

    # Create parents in batches
    if parent_payloads:
        if args.dry_run:
            print(f"[DRY-RUN] Would create {len(parent_payloads)} parent items")
        else:
            for start in range(0, len(parent_payloads), args.batch_size):
                batch = parent_payloads[start : start + args.batch_size]
                try:
                    resp = safe_create_items(zot, batch)
                    success = resp.get("success") or {}
                except Exception as e:
                    # Fallback to single-item creation on batch failure
                    success = {}
                    for i, one in enumerate(batch):
                        try:
                            r = safe_create_items(zot, [one])
                            if r.get("success"):
                                # Map single index to global rel index
                                single_key = next(iter(r["success"].values()))
                                success[str(i)] = single_key
                        except Exception:
                            # Skip and continue; report at the end
                            pass
                for rel_idx, key in success.items():
                    created_parents += 1
                    row_idx = parent_indices[start + int(rel_idx)]
                    parent_key_for_row[row_idx] = key

    # Create attachments in batches, now with parent keys if available
    attachment_payloads: List[Dict[str, object]] = []
    attachment_labels: List[str] = []
    for row_idx, att, chapter_label in pending_attachments:
        pk = parent_key_for_row.get(row_idx)
        if pk:
            att["parentItem"] = pk
        attachment_payloads.append(att)
        attachment_labels.append(chapter_label)

    if attachment_payloads:
        if args.dry_run:
            print(f"[DRY-RUN] Would create {len(attachment_payloads)} attachments")
        else:
            for start in range(0, len(attachment_payloads), args.batch_size):
                batch = attachment_payloads[start : start + args.batch_size]
                try:
                    resp = safe_create_items(zot, batch)
                    success = resp.get("success") or {}
                    failed = resp.get("failed") or {}
                except Exception:
                    # Fallback to single-item creation on batch failure
                    success = {}
                    failed = {}
                    for i, one in enumerate(batch):
                        try:
                            r = safe_create_items(zot, [one])
                            if r.get("success"):
                                success[str(i)] = next(iter(r["success"].values()))
                        except Exception as ee:
                            failed[str(i)] = str(getattr(getattr(ee, 'response', None), 'status_code', 'error'))
                created_attachments += len(success)
                if failed:
                    sys.stderr.write(f"Warnings while creating attachments: {failed}\n")
                # Per-collection stats
                for rel_idx in success.keys():
                    try:
                        ch = attachment_labels[start + int(rel_idx)]
                    except (ValueError, IndexError):
                        ch = "general"
                    created_by_collection[ch] += 1

    print("=== Zotero Linked Import (Best-Practices) ===")
    print(f"Parents created:    {created_parents}")
    print(f"Attachments created:{created_attachments}")
    print(f"Updated items:      {updated}")
    print(f"Skipped:            {skipped}")
    if created_by_collection:
        print("Created by collection:")
        for label, count in sorted(created_by_collection.items()):
            print(f"  - {label}: {count}")
    if missing_files:
        print("\nMissing files:")
        for name in missing_files:
            print(f"  - {name}")

    if args.base_dir:
        print("\nNote: Paths created relative to Linked Attachment Base Directory.")
        print("      Ensure Zotero is configured to the same base dir:")
        print(f"      {args.base_dir}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
